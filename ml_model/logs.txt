
==> Audit <==
|------------|------------------------------------|----------|-------|---------|----------------------|----------------------|
|  Command   |                Args                | Profile  | User  | Version |      Start Time      |       End Time       |
|------------|------------------------------------|----------|-------|---------|----------------------|----------------------|
| start      |                                    | minikube | satya | v1.36.0 | 23 Jun 25 03:23 CEST | 23 Jun 25 03:25 CEST |
| start      | --cpus=4 --memory=4096             | minikube | satya | v1.36.0 | 29 Jun 25 16:18 CEST | 29 Jun 25 16:19 CEST |
| service    | prometheus                         | minikube | satya | v1.36.0 | 29 Jun 25 19:10 CEST |                      |
| start      |                                    | minikube | satya | v1.36.0 | 29 Jun 25 19:11 CEST | 29 Jun 25 19:11 CEST |
| addons     | enable prometheus                  | minikube | satya | v1.36.0 | 29 Jun 25 19:11 CEST |                      |
| service    | prometheus                         | minikube | satya | v1.36.0 | 29 Jun 25 19:11 CEST |                      |
| service    | list                               | minikube | satya | v1.36.0 | 29 Jun 25 19:11 CEST | 29 Jun 25 19:11 CEST |
| node       |                                    | minikube | satya | v1.36.0 | 29 Jun 25 19:12 CEST |                      |
| node       | list                               | minikube | satya | v1.36.0 | 29 Jun 25 19:12 CEST |                      |
| image      | load flask-predictor               | minikube | satya | v1.36.0 | 29 Jun 25 21:15 CEST |                      |
| node       | list                               | minikube | satya | v1.36.0 | 29 Jun 25 21:27 CEST |                      |
| image      | load inference-service             | minikube | satya | v1.36.0 | 29 Jun 25 21:32 CEST |                      |
| image      | load inference-service             | minikube | satya | v1.36.0 | 29 Jun 25 21:37 CEST |                      |
| start      |                                    | minikube | satya | v1.36.0 | 29 Jun 25 23:14 CEST | 29 Jun 25 23:15 CEST |
| docker-env |                                    | minikube | satya | v1.36.0 | 29 Jun 25 23:16 CEST | 29 Jun 25 23:16 CEST |
| service    | inference-service                  | minikube | satya | v1.36.0 | 29 Jun 25 23:21 CEST |                      |
| service    | inference-service-849f8d958f-flt55 | minikube | satya | v1.36.0 | 29 Jun 25 23:24 CEST |                      |
| service    | list                               | minikube | satya | v1.36.0 | 29 Jun 25 23:24 CEST | 29 Jun 25 23:24 CEST |
| service    | inference-service                  | minikube | satya | v1.36.0 | 29 Jun 25 23:24 CEST |                      |
|------------|------------------------------------|----------|-------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/06/29 23:14:51
Running on machine: pop-os
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0629 23:14:51.002516    8913 out.go:345] Setting OutFile to fd 1 ...
I0629 23:14:51.002696    8913 out.go:397] isatty.IsTerminal(1) = true
I0629 23:14:51.002701    8913 out.go:358] Setting ErrFile to fd 2...
I0629 23:14:51.002707    8913 out.go:397] isatty.IsTerminal(2) = true
I0629 23:14:51.002816    8913 root.go:338] Updating PATH: /home/satya/.minikube/bin
W0629 23:14:51.002945    8913 root.go:314] Error reading config file at /home/satya/.minikube/config/config.json: open /home/satya/.minikube/config/config.json: no such file or directory
I0629 23:14:51.003411    8913 out.go:352] Setting JSON to false
I0629 23:14:51.005203    8913 start.go:130] hostinfo: {"hostname":"pop-os","uptime":305,"bootTime":1751231386,"procs":464,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"bookworm/sid","kernelVersion":"6.12.10-76061203-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"36e16f93-8573-7801-ab1b-21dd66ded873"}
I0629 23:14:51.005248    8913 start.go:140] virtualization: kvm host
I0629 23:14:51.009073    8913 out.go:177] üòÑ  minikube v1.36.0 on Debian bookworm/sid
I0629 23:14:51.016064    8913 notify.go:220] Checking for updates...
I0629 23:14:51.016426    8913 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0629 23:14:51.020912    8913 driver.go:404] Setting default libvirt URI to qemu:///system
I0629 23:14:51.041693    8913 docker.go:123] docker version: linux-28.3.0:Docker Engine - Community
I0629 23:14:51.041788    8913 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0629 23:14:51.298762    8913 info.go:266] docker info: {ID:160d31a3-2930-4a3f-b684-4a8513265bcd Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:44 SystemTime:2025-06-29 23:14:51.288627111 +0200 CEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.12.10-76061203-generic OperatingSystem:Pop!_OS 22.04 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:14496493568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:pop-os Labels:[] ExperimentalBuild:false ServerVersion:28.3.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.37.3]] Warnings:<nil>}}
I0629 23:14:51.298839    8913 docker.go:318] overlay module found
I0629 23:14:51.302201    8913 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0629 23:14:51.305018    8913 start.go:304] selected driver: docker
I0629 23:14:51.305025    8913 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/satya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0629 23:14:51.305073    8913 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0629 23:14:51.305151    8913 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0629 23:14:51.347702    8913 info.go:266] docker info: {ID:160d31a3-2930-4a3f-b684-4a8513265bcd Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:44 SystemTime:2025-06-29 23:14:51.338913225 +0200 CEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.12.10-76061203-generic OperatingSystem:Pop!_OS 22.04 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:14496493568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:pop-os Labels:[] ExperimentalBuild:false ServerVersion:28.3.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.37.3]] Warnings:<nil>}}
I0629 23:14:51.348988    8913 cni.go:84] Creating CNI manager for ""
I0629 23:14:51.349548    8913 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0629 23:14:51.349588    8913 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/satya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0629 23:14:51.352721    8913 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0629 23:14:51.355857    8913 cache.go:121] Beginning downloading kic base image for docker with docker
I0629 23:14:51.359138    8913 out.go:177] üöú  Pulling base image v0.0.47 ...
I0629 23:14:51.365008    8913 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0629 23:14:51.365523    8913 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0629 23:14:51.365561    8913 preload.go:146] Found local preload: /home/satya/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0629 23:14:51.365572    8913 cache.go:56] Caching tarball of preloaded images
I0629 23:14:51.365680    8913 preload.go:172] Found /home/satya/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0629 23:14:51.365690    8913 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0629 23:14:51.365762    8913 profile.go:143] Saving config to /home/satya/.minikube/profiles/minikube/config.json ...
I0629 23:14:51.382170    8913 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0629 23:14:51.382185    8913 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0629 23:14:51.382196    8913 cache.go:230] Successfully downloaded all kic artifacts
I0629 23:14:51.382216    8913 start.go:360] acquireMachinesLock for minikube: {Name:mkf14d2f9aa7d0f8c25951bb7b044854d811a977 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0629 23:14:51.382291    8913 start.go:364] duration metric: took 61.041¬µs to acquireMachinesLock for "minikube"
I0629 23:14:51.382304    8913 start.go:96] Skipping create...Using existing machine configuration
I0629 23:14:51.382312    8913 fix.go:54] fixHost starting: 
I0629 23:14:51.382494    8913 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0629 23:14:51.396991    8913 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0629 23:14:51.397008    8913 fix.go:138] unexpected machine state, will restart: <nil>
I0629 23:14:51.400288    8913 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0629 23:14:51.403493    8913 cli_runner.go:164] Run: docker start minikube
I0629 23:14:51.760292    8913 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0629 23:14:51.773230    8913 kic.go:430] container "minikube" state is running.
I0629 23:14:51.773524    8913 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0629 23:14:51.786885    8913 profile.go:143] Saving config to /home/satya/.minikube/profiles/minikube/config.json ...
I0629 23:14:51.787066    8913 machine.go:93] provisionDockerMachine start ...
I0629 23:14:51.787123    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:51.800795    8913 main.go:141] libmachine: Using SSH client type: native
I0629 23:14:51.801172    8913 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0629 23:14:51.801180    8913 main.go:141] libmachine: About to run SSH command:
hostname
I0629 23:14:51.801769    8913 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:43444->127.0.0.1:32768: read: connection reset by peer
I0629 23:14:54.930812    8913 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0629 23:14:54.930829    8913 ubuntu.go:169] provisioning hostname "minikube"
I0629 23:14:54.930899    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:54.947386    8913 main.go:141] libmachine: Using SSH client type: native
I0629 23:14:54.947559    8913 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0629 23:14:54.947567    8913 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0629 23:14:55.094929    8913 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0629 23:14:55.095003    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.108500    8913 main.go:141] libmachine: Using SSH client type: native
I0629 23:14:55.108680    8913 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0629 23:14:55.108691    8913 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0629 23:14:55.226809    8913 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0629 23:14:55.226861    8913 ubuntu.go:175] set auth options {CertDir:/home/satya/.minikube CaCertPath:/home/satya/.minikube/certs/ca.pem CaPrivateKeyPath:/home/satya/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/satya/.minikube/machines/server.pem ServerKeyPath:/home/satya/.minikube/machines/server-key.pem ClientKeyPath:/home/satya/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/satya/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/satya/.minikube}
I0629 23:14:55.226888    8913 ubuntu.go:177] setting up certificates
I0629 23:14:55.226904    8913 provision.go:84] configureAuth start
I0629 23:14:55.226976    8913 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0629 23:14:55.243589    8913 provision.go:143] copyHostCerts
I0629 23:14:55.244684    8913 exec_runner.go:144] found /home/satya/.minikube/ca.pem, removing ...
I0629 23:14:55.245326    8913 exec_runner.go:203] rm: /home/satya/.minikube/ca.pem
I0629 23:14:55.245590    8913 exec_runner.go:151] cp: /home/satya/.minikube/certs/ca.pem --> /home/satya/.minikube/ca.pem (1074 bytes)
I0629 23:14:55.245915    8913 exec_runner.go:144] found /home/satya/.minikube/cert.pem, removing ...
I0629 23:14:55.245921    8913 exec_runner.go:203] rm: /home/satya/.minikube/cert.pem
I0629 23:14:55.245949    8913 exec_runner.go:151] cp: /home/satya/.minikube/certs/cert.pem --> /home/satya/.minikube/cert.pem (1119 bytes)
I0629 23:14:55.246236    8913 exec_runner.go:144] found /home/satya/.minikube/key.pem, removing ...
I0629 23:14:55.246242    8913 exec_runner.go:203] rm: /home/satya/.minikube/key.pem
I0629 23:14:55.246271    8913 exec_runner.go:151] cp: /home/satya/.minikube/certs/key.pem --> /home/satya/.minikube/key.pem (1675 bytes)
I0629 23:14:55.246607    8913 provision.go:117] generating server cert: /home/satya/.minikube/machines/server.pem ca-key=/home/satya/.minikube/certs/ca.pem private-key=/home/satya/.minikube/certs/ca-key.pem org=satya.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0629 23:14:55.271583    8913 provision.go:177] copyRemoteCerts
I0629 23:14:55.271651    8913 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0629 23:14:55.271693    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.284982    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:14:55.374807    8913 ssh_runner.go:362] scp /home/satya/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0629 23:14:55.399058    8913 ssh_runner.go:362] scp /home/satya/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0629 23:14:55.419087    8913 ssh_runner.go:362] scp /home/satya/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0629 23:14:55.439024    8913 provision.go:87] duration metric: took 212.106674ms to configureAuth
I0629 23:14:55.439039    8913 ubuntu.go:193] setting minikube options for container-runtime
I0629 23:14:55.439148    8913 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0629 23:14:55.439203    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.452305    8913 main.go:141] libmachine: Using SSH client type: native
I0629 23:14:55.452486    8913 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0629 23:14:55.452493    8913 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0629 23:14:55.573820    8913 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0629 23:14:55.573833    8913 ubuntu.go:71] root file system type: overlay
I0629 23:14:55.573936    8913 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0629 23:14:55.574004    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.590023    8913 main.go:141] libmachine: Using SSH client type: native
I0629 23:14:55.590164    8913 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0629 23:14:55.590201    8913 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0629 23:14:55.723356    8913 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0629 23:14:55.723416    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.736633    8913 main.go:141] libmachine: Using SSH client type: native
I0629 23:14:55.736804    8913 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0629 23:14:55.736816    8913 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0629 23:14:55.862629    8913 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0629 23:14:55.862646    8913 machine.go:96] duration metric: took 4.075568922s to provisionDockerMachine
I0629 23:14:55.862656    8913 start.go:293] postStartSetup for "minikube" (driver="docker")
I0629 23:14:55.862670    8913 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0629 23:14:55.862739    8913 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0629 23:14:55.862790    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.876998    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:14:55.966360    8913 ssh_runner.go:195] Run: cat /etc/os-release
I0629 23:14:55.970433    8913 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0629 23:14:55.970451    8913 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0629 23:14:55.970471    8913 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0629 23:14:55.970479    8913 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0629 23:14:55.970488    8913 filesync.go:126] Scanning /home/satya/.minikube/addons for local assets ...
I0629 23:14:55.970720    8913 filesync.go:126] Scanning /home/satya/.minikube/files for local assets ...
I0629 23:14:55.970903    8913 start.go:296] duration metric: took 108.237149ms for postStartSetup
I0629 23:14:55.970954    8913 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0629 23:14:55.970995    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:55.984108    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:14:56.068493    8913 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0629 23:14:56.073494    8913 fix.go:56] duration metric: took 4.691173119s for fixHost
I0629 23:14:56.073510    8913 start.go:83] releasing machines lock for "minikube", held for 4.691208808s
I0629 23:14:56.073578    8913 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0629 23:14:56.088709    8913 ssh_runner.go:195] Run: cat /version.json
I0629 23:14:56.088766    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:56.088820    8913 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0629 23:14:56.088870    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:14:56.101712    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:14:56.102113    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:14:56.183243    8913 ssh_runner.go:195] Run: systemctl --version
I0629 23:14:56.394691    8913 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0629 23:14:56.399919    8913 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0629 23:14:56.420466    8913 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0629 23:14:56.420518    8913 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0629 23:14:56.428789    8913 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0629 23:14:56.428803    8913 start.go:495] detecting cgroup driver to use...
I0629 23:14:56.428826    8913 detect.go:190] detected "systemd" cgroup driver on host os
I0629 23:14:56.428911    8913 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0629 23:14:56.442884    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0629 23:14:56.451811    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0629 23:14:56.460681    8913 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0629 23:14:56.460741    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0629 23:14:56.469099    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0629 23:14:56.477781    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0629 23:14:56.486230    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0629 23:14:56.494775    8913 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0629 23:14:56.502626    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0629 23:14:56.511349    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0629 23:14:56.519852    8913 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0629 23:14:56.528485    8913 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0629 23:14:56.536628    8913 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0629 23:14:56.536670    8913 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0629 23:14:56.547625    8913 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0629 23:14:56.555211    8913 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0629 23:14:56.610753    8913 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0629 23:14:56.698031    8913 start.go:495] detecting cgroup driver to use...
I0629 23:14:56.698068    8913 detect.go:190] detected "systemd" cgroup driver on host os
I0629 23:14:56.698111    8913 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0629 23:14:56.715574    8913 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0629 23:14:56.715646    8913 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0629 23:14:56.733076    8913 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0629 23:14:56.747768    8913 ssh_runner.go:195] Run: which cri-dockerd
I0629 23:14:56.750714    8913 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0629 23:14:56.758294    8913 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0629 23:14:56.773738    8913 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0629 23:14:56.835354    8913 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0629 23:14:56.891546    8913 docker.go:587] configuring docker to use "systemd" as cgroup driver...
I0629 23:14:56.891603    8913 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0629 23:14:56.906919    8913 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0629 23:14:56.916496    8913 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0629 23:14:56.971340    8913 ssh_runner.go:195] Run: sudo systemctl restart docker
I0629 23:14:58.834886    8913 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.863523273s)
I0629 23:14:58.834928    8913 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0629 23:14:58.844949    8913 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0629 23:14:58.855136    8913 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0629 23:14:58.864851    8913 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0629 23:14:58.919685    8913 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0629 23:14:58.974314    8913 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0629 23:14:59.028578    8913 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0629 23:14:59.055073    8913 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0629 23:14:59.066324    8913 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0629 23:14:59.119477    8913 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0629 23:14:59.433946    8913 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0629 23:14:59.444142    8913 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0629 23:14:59.444233    8913 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0629 23:14:59.447172    8913 start.go:563] Will wait 60s for crictl version
I0629 23:14:59.447209    8913 ssh_runner.go:195] Run: which crictl
I0629 23:14:59.449958    8913 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0629 23:14:59.587593    8913 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0629 23:14:59.587642    8913 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0629 23:14:59.708693    8913 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0629 23:14:59.732585    8913 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0629 23:14:59.733347    8913 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0629 23:14:59.747385    8913 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0629 23:14:59.751112    8913 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0629 23:14:59.761135    8913 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/satya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0629 23:14:59.761205    8913 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0629 23:14:59.761248    8913 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0629 23:14:59.780142    8913 docker.go:702] Got preloaded images: -- stdout --
mongo:latest
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
nginx:latest
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I0629 23:14:59.780155    8913 docker.go:632] Images already preloaded, skipping extraction
I0629 23:14:59.780209    8913 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0629 23:14:59.795749    8913 docker.go:702] Got preloaded images: -- stdout --
mongo:latest
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
nginx:latest
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.16

-- /stdout --
I0629 23:14:59.795759    8913 cache_images.go:84] Images are preloaded, skipping loading
I0629 23:14:59.795767    8913 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0629 23:14:59.795848    8913 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0629 23:14:59.795891    8913 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0629 23:15:00.031063    8913 cni.go:84] Creating CNI manager for ""
I0629 23:15:00.031077    8913 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0629 23:15:00.031086    8913 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0629 23:15:00.031099    8913 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0629 23:15:00.031184    8913 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0629 23:15:00.031258    8913 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0629 23:15:00.040552    8913 binaries.go:44] Found k8s binaries, skipping transfer
I0629 23:15:00.040599    8913 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0629 23:15:00.050513    8913 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0629 23:15:00.068876    8913 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0629 23:15:00.086205    8913 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0629 23:15:00.104782    8913 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0629 23:15:00.107811    8913 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0629 23:15:00.117886    8913 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0629 23:15:00.176087    8913 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0629 23:15:00.203772    8913 certs.go:68] Setting up /home/satya/.minikube/profiles/minikube for IP: 192.168.49.2
I0629 23:15:00.203784    8913 certs.go:194] generating shared ca certs ...
I0629 23:15:00.203799    8913 certs.go:226] acquiring lock for ca certs: {Name:mk8d3abd4f027a9c65b7c1cddf9003d45530bb0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0629 23:15:00.204280    8913 certs.go:235] skipping valid "minikubeCA" ca cert: /home/satya/.minikube/ca.key
I0629 23:15:00.204719    8913 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/satya/.minikube/proxy-client-ca.key
I0629 23:15:00.204729    8913 certs.go:256] generating profile certs ...
I0629 23:15:00.205120    8913 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/satya/.minikube/profiles/minikube/client.key
I0629 23:15:00.205371    8913 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/satya/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0629 23:15:00.205639    8913 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/satya/.minikube/profiles/minikube/proxy-client.key
I0629 23:15:00.205742    8913 certs.go:484] found cert: /home/satya/.minikube/certs/ca-key.pem (1679 bytes)
I0629 23:15:00.205774    8913 certs.go:484] found cert: /home/satya/.minikube/certs/ca.pem (1074 bytes)
I0629 23:15:00.205791    8913 certs.go:484] found cert: /home/satya/.minikube/certs/cert.pem (1119 bytes)
I0629 23:15:00.205807    8913 certs.go:484] found cert: /home/satya/.minikube/certs/key.pem (1675 bytes)
I0629 23:15:00.206200    8913 ssh_runner.go:362] scp /home/satya/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0629 23:15:00.228473    8913 ssh_runner.go:362] scp /home/satya/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0629 23:15:00.250698    8913 ssh_runner.go:362] scp /home/satya/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0629 23:15:00.271972    8913 ssh_runner.go:362] scp /home/satya/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0629 23:15:00.293422    8913 ssh_runner.go:362] scp /home/satya/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0629 23:15:00.315475    8913 ssh_runner.go:362] scp /home/satya/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0629 23:15:00.336896    8913 ssh_runner.go:362] scp /home/satya/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0629 23:15:00.358120    8913 ssh_runner.go:362] scp /home/satya/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0629 23:15:00.379908    8913 ssh_runner.go:362] scp /home/satya/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0629 23:15:00.406036    8913 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0629 23:15:00.421378    8913 ssh_runner.go:195] Run: openssl version
I0629 23:15:00.431031    8913 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0629 23:15:00.440639    8913 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0629 23:15:00.444011    8913 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 23 01:24 /usr/share/ca-certificates/minikubeCA.pem
I0629 23:15:00.444046    8913 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0629 23:15:00.449348    8913 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0629 23:15:00.457400    8913 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0629 23:15:00.460335    8913 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0629 23:15:00.465875    8913 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0629 23:15:00.471893    8913 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0629 23:15:00.477490    8913 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0629 23:15:00.483068    8913 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0629 23:15:00.488162    8913 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0629 23:15:00.493572    8913 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3400 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/satya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0629 23:15:00.493655    8913 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0629 23:15:00.508902    8913 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0629 23:15:00.517708    8913 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0629 23:15:00.517719    8913 kubeadm.go:589] restartPrimaryControlPlane start ...
I0629 23:15:00.517761    8913 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0629 23:15:00.527051    8913 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0629 23:15:00.528688    8913 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0629 23:15:00.549229    8913 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0629 23:15:00.558665    8913 kubeadm.go:626] The running cluster does not require reconfiguration: 192.168.49.2
I0629 23:15:00.558687    8913 kubeadm.go:593] duration metric: took 40.960428ms to restartPrimaryControlPlane
I0629 23:15:00.558696    8913 kubeadm.go:394] duration metric: took 65.130275ms to StartCluster
I0629 23:15:00.558708    8913 settings.go:142] acquiring lock: {Name:mkc9b611e54c9259e18bbc29d8e2a6c6e134d251 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0629 23:15:00.558795    8913 settings.go:150] Updating kubeconfig:  /home/satya/.kube/config
I0629 23:15:00.559237    8913 lock.go:35] WriteFile acquiring /home/satya/.kube/config: {Name:mkba1b215148fdd36d44f118dcfb363896e870c1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0629 23:15:00.559424    8913 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0629 23:15:00.559499    8913 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0629 23:15:00.559596    8913 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0629 23:15:00.559601    8913 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0629 23:15:00.559619    8913 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0629 23:15:00.559625    8913 addons.go:247] addon storage-provisioner should already be in state true
I0629 23:15:00.559625    8913 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0629 23:15:00.559633    8913 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0629 23:15:00.559641    8913 host.go:66] Checking if "minikube" exists ...
I0629 23:15:00.559826    8913 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0629 23:15:00.559880    8913 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0629 23:15:00.565156    8913 out.go:177] üîé  Verifying Kubernetes components...
I0629 23:15:00.568994    8913 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0629 23:15:00.576781    8913 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0629 23:15:00.576857    8913 addons.go:247] addon default-storageclass should already be in state true
I0629 23:15:00.576877    8913 host.go:66] Checking if "minikube" exists ...
I0629 23:15:00.577389    8913 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0629 23:15:00.578854    8913 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0629 23:15:00.582282    8913 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0629 23:15:00.582294    8913 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0629 23:15:00.582350    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:15:00.592632    8913 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0629 23:15:00.592647    8913 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0629 23:15:00.592711    8913 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0629 23:15:00.597001    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:15:00.609013    8913 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/satya/.minikube/machines/minikube/id_rsa Username:docker}
I0629 23:15:00.651134    8913 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0629 23:15:00.662419    8913 api_server.go:52] waiting for apiserver process to appear ...
I0629 23:15:00.662500    8913 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0629 23:15:00.696559    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0629 23:15:00.704772    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0629 23:15:00.948140    8913 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0629 23:15:00.948166    8913 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:00.948169    8913 retry.go:31] will retry after 204.527152ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:00.948180    8913 retry.go:31] will retry after 231.273305ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.153343    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0629 23:15:01.162654    8913 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0629 23:15:01.180180    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0629 23:15:01.214152    8913 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.214181    8913 retry.go:31] will retry after 357.617851ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0629 23:15:01.228303    8913 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.228333    8913 retry.go:31] will retry after 478.137476ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.572658    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0629 23:15:01.625282    8913 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.625302    8913 retry.go:31] will retry after 594.147842ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.663578    8913 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0629 23:15:01.674570    8913 api_server.go:72] duration metric: took 1.115120827s to wait for apiserver process to appear ...
I0629 23:15:01.674585    8913 api_server.go:88] waiting for apiserver healthz status ...
I0629 23:15:01.674615    8913 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0629 23:15:01.674883    8913 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0629 23:15:01.707183    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0629 23:15:01.755363    8913 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:01.755389    8913 retry.go:31] will retry after 456.932691ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0629 23:15:02.176513    8913 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0629 23:15:02.213183    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0629 23:15:02.220471    8913 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0629 23:15:03.369340    8913 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0629 23:15:03.369354    8913 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0629 23:15:03.369374    8913 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0629 23:15:03.386627    8913 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0629 23:15:03.386655    8913 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0629 23:15:03.423417    8913 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.21020559s)
I0629 23:15:03.675605    8913 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0629 23:15:03.678646    8913 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0629 23:15:03.678659    8913 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0629 23:15:03.705526    8913 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.485028949s)
I0629 23:15:03.709770    8913 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0629 23:15:03.713134    8913 addons.go:514] duration metric: took 3.153631502s for enable addons: enabled=[default-storageclass storage-provisioner]
I0629 23:15:04.175521    8913 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0629 23:15:04.179130    8913 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0629 23:15:04.179146    8913 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0629 23:15:04.676771    8913 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0629 23:15:04.680760    8913 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0629 23:15:04.681828    8913 api_server.go:141] control plane version: v1.33.1
I0629 23:15:04.681848    8913 api_server.go:131] duration metric: took 3.007252948s to wait for apiserver health ...
I0629 23:15:04.681857    8913 system_pods.go:43] waiting for kube-system pods to appear ...
I0629 23:15:04.691394    8913 system_pods.go:59] 7 kube-system pods found
I0629 23:15:04.691420    8913 system_pods.go:61] "coredns-674b8bbfcf-b95zd" [928b105c-03e0-4dda-97b2-fb17223cac6f] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0629 23:15:04.691428    8913 system_pods.go:61] "etcd-minikube" [68b5b197-a72b-4cba-afeb-32add6da9a9c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0629 23:15:04.691435    8913 system_pods.go:61] "kube-apiserver-minikube" [008cebf6-409f-4aa0-b06c-86678a66465c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0629 23:15:04.691442    8913 system_pods.go:61] "kube-controller-manager-minikube" [690e07cf-d6bf-4e24-bc0c-17fdbd3ab33b] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0629 23:15:04.691449    8913 system_pods.go:61] "kube-proxy-sfj65" [3397e75e-93c1-45b4-8e8d-7c52ce1b00cf] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0629 23:15:04.691471    8913 system_pods.go:61] "kube-scheduler-minikube" [1605489a-294f-4be6-ab7a-a1b86d483bc5] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0629 23:15:04.691477    8913 system_pods.go:61] "storage-provisioner" [f91ba068-640a-451b-9592-a8095f4f1536] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0629 23:15:04.691486    8913 system_pods.go:74] duration metric: took 9.619022ms to wait for pod list to return data ...
I0629 23:15:04.691526    8913 kubeadm.go:578] duration metric: took 4.132053419s to wait for: map[apiserver:true system_pods:true]
I0629 23:15:04.691545    8913 node_conditions.go:102] verifying NodePressure condition ...
I0629 23:15:04.694276    8913 node_conditions.go:122] node storage ephemeral capacity is 204102700Ki
I0629 23:15:04.694289    8913 node_conditions.go:123] node cpu capacity is 16
I0629 23:15:04.694308    8913 node_conditions.go:105] duration metric: took 2.754907ms to run NodePressure ...
I0629 23:15:04.694320    8913 start.go:241] waiting for startup goroutines ...
I0629 23:15:04.694328    8913 start.go:246] waiting for cluster config update ...
I0629 23:15:04.694338    8913 start.go:255] writing updated cluster config ...
I0629 23:15:04.694588    8913 ssh_runner.go:195] Run: rm -f paused
I0629 23:15:05.038863    8913 start.go:607] kubectl: 1.33.2, cluster: 1.33.1 (minor skew: 0)
I0629 23:15:05.050578    8913 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.794682100Z" level=warning msg="error locating sandbox id d3a0ca89c74069e3062155d9c4b8ff430b91f38b41479cfef4e48d397a35feba: sandbox d3a0ca89c74069e3062155d9c4b8ff430b91f38b41479cfef4e48d397a35feba not found"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.794718557Z" level=warning msg="error locating sandbox id b29b8167889c111f2703784157f2a42663af08e1a5855ebb1880247fff2427b8: sandbox b29b8167889c111f2703784157f2a42663af08e1a5855ebb1880247fff2427b8 not found"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.794737205Z" level=warning msg="error locating sandbox id 51b0ff95f3f9d711137eda390db18f4d2c35251b23ac122429b6db6e3d14292f: sandbox 51b0ff95f3f9d711137eda390db18f4d2c35251b23ac122429b6db6e3d14292f not found"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.794777154Z" level=warning msg="error locating sandbox id 426c8ca74d9ab3887adfd32dc7e4487d0b61492d3c80b6f8a8380a5e72bd8224: sandbox 426c8ca74d9ab3887adfd32dc7e4487d0b61492d3c80b6f8a8380a5e72bd8224 not found"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.794844901Z" level=warning msg="error locating sandbox id f34e56fd98a1cb0da99b3bab1e63c2b17275a1729759f137f7ba8d6adebd1f1a: sandbox f34e56fd98a1cb0da99b3bab1e63c2b17275a1729759f137f7ba8d6adebd1f1a not found"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.794862082Z" level=warning msg="error locating sandbox id 9e37a0ac2641e8d82b75df632892b7960eaf7e4204c4b32dbd6dfba11fb4452f: sandbox 9e37a0ac2641e8d82b75df632892b7960eaf7e4204c4b32dbd6dfba11fb4452f not found"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.795086134Z" level=info msg="Loading containers: done."
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.811838371Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.811918899Z" level=info msg="Initializing buildkit"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.829250543Z" level=info msg="Completed buildkit initialization"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.833221254Z" level=info msg="Daemon has completed initialization"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.833277477Z" level=info msg="API listen on /var/run/docker.sock"
Jun 29 21:14:58 minikube dockerd[1101]: time="2025-06-29T21:14:58.833298010Z" level=info msg="API listen on [::]:2376"
Jun 29 21:14:58 minikube systemd[1]: Started Docker Application Container Engine.
Jun 29 21:14:59 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Start docker client with request timeout 0s"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Loaded network plugin cni"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Setting cgroupDriver systemd"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 29 21:14:59 minikube cri-dockerd[1437]: time="2025-06-29T21:14:59Z" level=info msg="Start cri-dockerd grpc backend"
Jun 29 21:14:59 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-b95zd_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5d1e3efeb8a855aa91a5a1fd36ee764c279eb83e121e7b3bebdc602d066b4959\""
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-b95zd_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5f3b0c265cff6936a43988347212856c6d07d3ba407f5a56f5b286f8bf9dbaac\""
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-687d5fb844-p6fmv_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"400e94c9dc1ad46655f226503d328fa67a6c7afd886ebc2798d7b173c2476095\""
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-687d5fb844-p6fmv_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"bbc9d7916f4846fdf782b5e1cd4b6ba066b9f7ba476ce349be530f908b62b1eb\""
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-687d5fb844-fkk78_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d2f8d58aab5b8a2da2a0af50fd68873f02025458e709930157f2fa10a8126466\""
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-687d5fb844-fkk78_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fdce390423de1dd72afa578621babbf33edd8a01a247d4f24b58a3c8d0474e54\""
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"fa55e7d83223fa62e6936931a4031960755fe4ab57eff33818919ba7856731ed\". Proceed without further sandbox information."
Jun 29 21:15:00 minikube cri-dockerd[1437]: time="2025-06-29T21:15:00Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a14acfbe088ea5f31ec9f0eeff8c5f4eb5743f9ee8ff6a7dcf272f1a308adc9b\". Proceed without further sandbox information."
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a2b9e6f3e21d91a55db2ad33e7b45b53c1bf93228b8d3e4eafab97a00e9c4b16\". Proceed without further sandbox information."
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6d117a08e82efcdb478963f87c1ca60e1b8a26967042c26b05627326dd01f6e/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/741406bf80d2c8f59fd1d2a30954ea6022ef44d08c30fab0cbc0429e85fb2f8d/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f309782a0d31cb7d2e4c93b9374f579ee20dc3e49c42f8181da9336ddfd559c3/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d385fa909e0fc3dc26f267fb1c21398391201c479ebb22307c541748aa450488/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-b95zd_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5d1e3efeb8a855aa91a5a1fd36ee764c279eb83e121e7b3bebdc602d066b4959\""
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-687d5fb844-p6fmv_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"400e94c9dc1ad46655f226503d328fa67a6c7afd886ebc2798d7b173c2476095\""
Jun 29 21:15:01 minikube cri-dockerd[1437]: time="2025-06-29T21:15:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-687d5fb844-fkk78_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d2f8d58aab5b8a2da2a0af50fd68873f02025458e709930157f2fa10a8126466\""
Jun 29 21:15:03 minikube cri-dockerd[1437]: time="2025-06-29T21:15:03Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 29 21:15:05 minikube cri-dockerd[1437]: time="2025-06-29T21:15:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/65761c3f13d365ec3c157e035ca2474bd5ec9a00516f0af3f1922d3851de3814/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 29 21:15:05 minikube cri-dockerd[1437]: time="2025-06-29T21:15:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/63f970e66e3b27860e0335874ea174be85cf4e51edf224854a858d188f49cce3/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Jun 29 21:15:05 minikube cri-dockerd[1437]: time="2025-06-29T21:15:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d464c69f8b06eeba1bfe36eceae59eb347355806646e9e4e1446005ca1a61eaf/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 29 21:15:05 minikube cri-dockerd[1437]: time="2025-06-29T21:15:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6ecee492f1ccd20ea5b3f36e8f5a7088c6f21ea4ada1e2e3a6f13add9d7413c2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 29 21:15:05 minikube cri-dockerd[1437]: time="2025-06-29T21:15:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2fa4101ddb5c5140c6c1fe3a4d19503dcb311a048e0f9a7c1343c8abd5c8f9e8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 29 21:15:06 minikube dockerd[1101]: time="2025-06-29T21:15:06.674860648Z" level=info msg="ignoring event" container=39b6f7099fd61d77230a565042d9046cf79c70d00d793c453811210c56362102 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 29 21:21:09 minikube cri-dockerd[1437]: time="2025-06-29T21:21:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/301a76b6d22efa975759f63a8fc9646a3fa71ed59ea8eeb9f12b50c92595448d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 29 21:21:11 minikube dockerd[1101]: time="2025-06-29T21:21:11.972362644Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 29 21:21:11 minikube dockerd[1101]: time="2025-06-29T21:21:11.972436675Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 29 21:21:29 minikube dockerd[1101]: time="2025-06-29T21:21:29.784426600Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 29 21:21:29 minikube dockerd[1101]: time="2025-06-29T21:21:29.784506847Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 29 21:22:01 minikube dockerd[1101]: time="2025-06-29T21:22:01.835737808Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 29 21:22:01 minikube dockerd[1101]: time="2025-06-29T21:22:01.835810303Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 29 21:22:44 minikube dockerd[1101]: time="2025-06-29T21:22:44.680797852Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 29 21:22:44 minikube dockerd[1101]: time="2025-06-29T21:22:44.680854423Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 29 21:24:17 minikube dockerd[1101]: time="2025-06-29T21:24:17.927227345Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 29 21:24:17 minikube dockerd[1101]: time="2025-06-29T21:24:17.927270577Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4e2a4065be905       6e38f40d628db       9 minutes ago       Running             storage-provisioner       8                   63f970e66e3b2       storage-provisioner
1fbc3a6e0c59d       dfcfd8e9a5d38       9 minutes ago       Running             nginx                     3                   2fa4101ddb5c5       nginx-deployment-687d5fb844-p6fmv
d5049fb1b14ce       dfcfd8e9a5d38       9 minutes ago       Running             nginx                     3                   6ecee492f1ccd       nginx-deployment-687d5fb844-fkk78
dead37a0f6a2b       1cf5f116067c6       9 minutes ago       Running             coredns                   3                   d464c69f8b06e       coredns-674b8bbfcf-b95zd
39b6f7099fd61       6e38f40d628db       9 minutes ago       Exited              storage-provisioner       7                   63f970e66e3b2       storage-provisioner
e534b63e6b373       b79c189b052cd       9 minutes ago       Running             kube-proxy                3                   65761c3f13d36       kube-proxy-sfj65
d4d21952546b5       398c985c0d950       9 minutes ago       Running             kube-scheduler            3                   d385fa909e0fc       kube-scheduler-minikube
df58b0bc3bb3a       c6ab243b29f82       9 minutes ago       Running             kube-apiserver            3                   a6d117a08e82e       kube-apiserver-minikube
dba765f3181e8       ef43894fa110c       9 minutes ago       Running             kube-controller-manager   3                   f309782a0d31c       kube-controller-manager-minikube
ebe86543c439b       499038711c081       9 minutes ago       Running             etcd                      3                   741406bf80d2c       etcd-minikube
bdd8ce0f11755       1cf5f116067c6       4 hours ago         Exited              coredns                   2                   5d1e3efeb8a85       coredns-674b8bbfcf-b95zd
8be198d312818       b79c189b052cd       4 hours ago         Exited              kube-proxy                2                   dd35d1a33d372       kube-proxy-sfj65
f3e8fca29a598       dfcfd8e9a5d38       4 hours ago         Exited              nginx                     2                   d2f8d58aab5b8       nginx-deployment-687d5fb844-fkk78
c7f67fcc56103       dfcfd8e9a5d38       4 hours ago         Exited              nginx                     2                   400e94c9dc1ad       nginx-deployment-687d5fb844-p6fmv
ba10003638a40       c6ab243b29f82       4 hours ago         Exited              kube-apiserver            2                   a5bbc77604cc0       kube-apiserver-minikube
97f5e6d9e8ac8       398c985c0d950       4 hours ago         Exited              kube-scheduler            2                   18eeaa138f890       kube-scheduler-minikube
cecb88eb1926b       499038711c081       4 hours ago         Exited              etcd                      2                   325d33905d015       etcd-minikube
c036a04edc600       ef43894fa110c       4 hours ago         Exited              kube-controller-manager   2                   59df2286c6253       kube-controller-manager-minikube


==> coredns [bdd8ce0f1175] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:37239 - 64324 "HINFO IN 1701355218889314272.1717046612831382896. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.08228217s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> coredns [dead37a0f6a2] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:48207 - 59855 "HINFO IN 8878975336228512888.49150935600601655. udp 55 false 512" NXDOMAIN qr,rd,ra 130 0.005755378s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_23T03_25_03_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 23 Jun 2025 01:24:58 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 29 Jun 2025 21:24:55 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 29 Jun 2025 21:20:09 +0000   Mon, 23 Jun 2025 01:24:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 29 Jun 2025 21:20:09 +0000   Mon, 23 Jun 2025 01:24:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 29 Jun 2025 21:20:09 +0000   Mon, 23 Jun 2025 01:24:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 29 Jun 2025 21:20:09 +0000   Mon, 23 Jun 2025 01:24:59 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  204102700Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             14156732Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  204102700Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             14156732Ki
  pods:               110
System Info:
  Machine ID:                 76d2aa95bd6f4c07899b2c243a7687d7
  System UUID:                1f76bdc4-3079-40c0-b873-13a4dc4e814d
  Boot ID:                    85eb0af2-cfcc-495d-9948-bd417ef6f0b9
  Kernel Version:             6.12.10-76061203-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                  ------------  ----------  ---------------  -------------  ---
  default                     inference-service-849f8d958f-flt55    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m47s
  default                     nginx-deployment-687d5fb844-fkk78     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d19h
  default                     nginx-deployment-687d5fb844-p6fmv     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d19h
  kube-system                 coredns-674b8bbfcf-b95zd              100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     6d19h
  kube-system                 etcd-minikube                         100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         6d19h
  kube-system                 kube-apiserver-minikube               250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d19h
  kube-system                 kube-controller-manager-minikube      200m (1%)     0 (0%)      0 (0%)           0 (0%)         6d19h
  kube-system                 kube-proxy-sfj65                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d19h
  kube-system                 kube-scheduler-minikube               100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d19h
  kube-system                 storage-provisioner                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d19h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 9m49s                  kube-proxy       
  Normal   Starting                 9m55s                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  9m55s (x8 over 9m55s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    9m55s (x8 over 9m55s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     9m55s (x7 over 9m55s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  9m55s                  kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 9m52s                  kubelet          Node minikube has been rebooted, boot id: 85eb0af2-cfcc-495d-9948-bd417ef6f0b9
  Normal   RegisteredNode           9m49s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun29 21:09] TSC synchronization [CPU#0 -> CPU#2]:
[  +0.000000] Measured 7488098324 cycles TSC warp between CPUs, turning off TSC clock.
[  +0.001173]   #1  #3  #5  #7  #9 #11 #13 #15
[  +0.065986] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PCI0.PB2], AE_NOT_FOUND (20240827/dswload2-162)
[  +0.000021] ACPI Error: AE_NOT_FOUND, During name lookup/catalog (20240827/psobject-220)
[  +0.316493] tpm tpm0: tpm_read_log_acpi: Failed to map ACPI memory
[  +0.007761] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.001947] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000090] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.828801] usb: port power management may be unreliable
[  +4.203258] integrity: Problem loading X.509 certificate -65
[  +0.044749] Unstable clock detected, switching default tracing clock to "global"
              If you want to keep using the local clock, then add:
                "trace_clock=local"
              on the kernel command line
[  +0.610036] nvme nvme0: missing or invalid SUBNQN field.
[  +4.533334] system76_acpi: loading out-of-tree module taints kernel.
[  +1.355008] block nvme0n1: the capability attribute has been deprecated.
[  +0.062409] systemd[1]: Configuration file /run/systemd/system/netplan-ovs-cleanup.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[  +0.288984] ideapad_acpi VPC2004:00: Unknown keyboard type: 1
[  +0.124482] FAT-fs (nvme0n1p1): Volume was not properly unmounted. Some data may be corrupt. Please run fsck.
[  +0.192720] nvidia: module license 'NVIDIA' taints kernel.
[  +0.000005] Disabling lock debugging due to kernel taint
[  +0.000011] nvidia: module license taints kernel.

[  +0.191507] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  565.77  Wed Nov 27 23:33:08 UTC 2024
[Jun29 21:10] nvidia-modeset: WARNING: GPU:0: Unable to read EDID for display device DP-4
[  +0.013091] nvidia-modeset: WARNING: GPU:0: Unable to read EDID for display device DP-4
[  +0.065444] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.
[  +5.308622] kauditd_printk_skb: 20 callbacks suppressed
[ +11.227620] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211


==> etcd [cecb88eb1926] <==
{"level":"info","ts":"2025-06-29T19:16:54.080268Z","caller":"traceutil/trace.go:171","msg":"trace[881270338] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12553; }","duration":"104.99722ms","start":"2025-06-29T19:16:53.975262Z","end":"2025-06-29T19:16:54.080259Z","steps":["trace[881270338] 'agreement among raft nodes before linearized reading'  (duration: 104.932337ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:16:56.618519Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"222.424105ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-29T19:16:56.618628Z","caller":"traceutil/trace.go:171","msg":"trace[520460873] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12555; }","duration":"222.503655ms","start":"2025-06-29T19:16:56.396068Z","end":"2025-06-29T19:16:56.618573Z","steps":["trace[520460873] 'range keys from in-memory index tree'  (duration: 222.290707ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:16:58.801107Z","caller":"traceutil/trace.go:171","msg":"trace[2066715895] linearizableReadLoop","detail":"{readStateIndex:15621; appliedIndex:15620; }","duration":"404.413471ms","start":"2025-06-29T19:16:58.396671Z","end":"2025-06-29T19:16:58.801085Z","steps":["trace[2066715895] 'read index received'  (duration: 404.209253ms)","trace[2066715895] 'applied index is now lower than readState.Index'  (duration: 197.932¬µs)"],"step_count":2}
{"level":"info","ts":"2025-06-29T19:16:58.801146Z","caller":"traceutil/trace.go:171","msg":"trace[661784819] transaction","detail":"{read_only:false; response_revision:12556; number_of_response:1; }","duration":"813.124653ms","start":"2025-06-29T19:16:57.988002Z","end":"2025-06-29T19:16:58.801127Z","steps":["trace[661784819] 'process raft request'  (duration: 812.918481ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:16:58.801203Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"404.550012ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-06-29T19:16:58.801228Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:16:57.987990Z","time spent":"813.188559ms","remote":"127.0.0.1:56102","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12555 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-06-29T19:16:58.801247Z","caller":"traceutil/trace.go:171","msg":"trace[645032974] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12556; }","duration":"404.605326ms","start":"2025-06-29T19:16:58.396630Z","end":"2025-06-29T19:16:58.801236Z","steps":["trace[645032974] 'agreement among raft nodes before linearized reading'  (duration: 404.519561ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:16:59.588451Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.351018ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038264092164287 > lease_revoke:<id:70cc97bcabff0075>","response":"size:29"}
{"level":"info","ts":"2025-06-29T19:16:59.588525Z","caller":"traceutil/trace.go:171","msg":"trace[302483102] linearizableReadLoop","detail":"{readStateIndex:15622; appliedIndex:15621; }","duration":"192.327899ms","start":"2025-06-29T19:16:59.396184Z","end":"2025-06-29T19:16:59.588512Z","steps":["trace[302483102] 'read index received'  (duration: 11.847535ms)","trace[302483102] 'applied index is now lower than readState.Index'  (duration: 180.476453ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-29T19:16:59.588617Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.462764ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-29T19:16:59.588672Z","caller":"traceutil/trace.go:171","msg":"trace[782522989] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12556; }","duration":"192.496638ms","start":"2025-06-29T19:16:59.396136Z","end":"2025-06-29T19:16:59.588633Z","steps":["trace[782522989] 'agreement among raft nodes before linearized reading'  (duration: 192.41632ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:17:00.845355Z","caller":"traceutil/trace.go:171","msg":"trace[111068134] transaction","detail":"{read_only:false; response_revision:12557; number_of_response:1; }","duration":"196.220739ms","start":"2025-06-29T19:17:00.649116Z","end":"2025-06-29T19:17:00.845337Z","steps":["trace[111068134] 'process raft request'  (duration: 195.99976ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:01.415084Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"473.091725ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038264092164293 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12556 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-06-29T19:17:01.415169Z","caller":"traceutil/trace.go:171","msg":"trace[1051062393] linearizableReadLoop","detail":"{readStateIndex:15624; appliedIndex:15623; }","duration":"439.37618ms","start":"2025-06-29T19:17:00.975781Z","end":"2025-06-29T19:17:01.415157Z","steps":["trace[1051062393] 'read index received'  (duration: 88.908¬µs)","trace[1051062393] 'applied index is now lower than readState.Index'  (duration: 439.282522ms)"],"step_count":2}
{"level":"info","ts":"2025-06-29T19:17:01.415198Z","caller":"traceutil/trace.go:171","msg":"trace[1273740885] transaction","detail":"{read_only:false; response_revision:12558; number_of_response:1; }","duration":"566.612619ms","start":"2025-06-29T19:17:00.848560Z","end":"2025-06-29T19:17:01.415173Z","steps":["trace[1273740885] 'process raft request'  (duration: 93.377788ms)","trace[1273740885] 'compare'  (duration: 472.897285ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-29T19:17:01.415279Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"439.503152ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-06-29T19:17:01.415279Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:00.848539Z","time spent":"566.706206ms","remote":"127.0.0.1:56102","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12556 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-06-29T19:17:01.415303Z","caller":"traceutil/trace.go:171","msg":"trace[1693581512] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12558; }","duration":"439.576277ms","start":"2025-06-29T19:17:00.975719Z","end":"2025-06-29T19:17:01.415295Z","steps":["trace[1693581512] 'agreement among raft nodes before linearized reading'  (duration: 439.523686ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:01.415320Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:00.975675Z","time spent":"439.640112ms","remote":"127.0.0.1:55914","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-06-29T19:17:03.649183Z","caller":"traceutil/trace.go:171","msg":"trace[193201472] transaction","detail":"{read_only:false; response_revision:12559; number_of_response:1; }","duration":"226.78237ms","start":"2025-06-29T19:17:03.422380Z","end":"2025-06-29T19:17:03.649161Z","steps":["trace[193201472] 'process raft request'  (duration: 226.564185ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:17:04.889732Z","caller":"traceutil/trace.go:171","msg":"trace[62538133] linearizableReadLoop","detail":"{readStateIndex:15628; appliedIndex:15627; }","duration":"493.428035ms","start":"2025-06-29T19:17:04.396279Z","end":"2025-06-29T19:17:04.889707Z","steps":["trace[62538133] 'read index received'  (duration: 493.113747ms)","trace[62538133] 'applied index is now lower than readState.Index'  (duration: 308.98¬µs)"],"step_count":2}
{"level":"info","ts":"2025-06-29T19:17:04.889831Z","caller":"traceutil/trace.go:171","msg":"trace[2108178991] transaction","detail":"{read_only:false; response_revision:12561; number_of_response:1; }","duration":"545.815702ms","start":"2025-06-29T19:17:04.343991Z","end":"2025-06-29T19:17:04.889808Z","steps":["trace[2108178991] 'process raft request'  (duration: 545.462163ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:04.889919Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"493.676811ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-06-29T19:17:04.889947Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.54245ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-06-29T19:17:04.889980Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:04.343972Z","time spent":"545.920324ms","remote":"127.0.0.1:56200","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:12553 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-06-29T19:17:04.890012Z","caller":"traceutil/trace.go:171","msg":"trace[88352166] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:12561; }","duration":"121.636527ms","start":"2025-06-29T19:17:04.768356Z","end":"2025-06-29T19:17:04.889992Z","steps":["trace[88352166] 'agreement among raft nodes before linearized reading'  (duration: 121.528552ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:17:04.889992Z","caller":"traceutil/trace.go:171","msg":"trace[276769541] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12561; }","duration":"493.770399ms","start":"2025-06-29T19:17:04.396202Z","end":"2025-06-29T19:17:04.889973Z","steps":["trace[276769541] 'agreement among raft nodes before linearized reading'  (duration: 493.639516ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:05.858097Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"461.292241ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-29T19:17:05.858153Z","caller":"traceutil/trace.go:171","msg":"trace[1848012587] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12561; }","duration":"461.366483ms","start":"2025-06-29T19:17:05.396773Z","end":"2025-06-29T19:17:05.858139Z","steps":["trace[1848012587] 'range keys from in-memory index tree'  (duration: 461.140056ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:05.858253Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"206.197633ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-06-29T19:17:05.858307Z","caller":"traceutil/trace.go:171","msg":"trace[1119152983] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:12561; }","duration":"206.278719ms","start":"2025-06-29T19:17:05.652014Z","end":"2025-06-29T19:17:05.858293Z","steps":["trace[1119152983] 'range keys from in-memory index tree'  (duration: 206.062419ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:17:06.003219Z","caller":"traceutil/trace.go:171","msg":"trace[312653789] transaction","detail":"{read_only:false; response_revision:12562; number_of_response:1; }","duration":"141.589826ms","start":"2025-06-29T19:17:05.861606Z","end":"2025-06-29T19:17:06.003195Z","steps":["trace[312653789] 'process raft request'  (duration: 141.346567ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:17:08.177622Z","caller":"traceutil/trace.go:171","msg":"trace[1455994185] transaction","detail":"{read_only:false; response_revision:12563; number_of_response:1; }","duration":"168.319938ms","start":"2025-06-29T19:17:08.009283Z","end":"2025-06-29T19:17:08.177602Z","steps":["trace[1455994185] 'process raft request'  (duration: 168.147848ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:10.438451Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"650.207393ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038264092164331 > lease_revoke:<id:70cc97bcabff00a3>","response":"size:29"}
{"level":"info","ts":"2025-06-29T19:17:10.438534Z","caller":"traceutil/trace.go:171","msg":"trace[2037100132] linearizableReadLoop","detail":"{readStateIndex:15631; appliedIndex:15630; }","duration":"462.178672ms","start":"2025-06-29T19:17:09.976339Z","end":"2025-06-29T19:17:10.438517Z","steps":["trace[2037100132] 'read index received'  (duration: 41.486¬µs)","trace[2037100132] 'applied index is now lower than readState.Index'  (duration: 462.133345ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-29T19:17:10.438705Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"462.357398ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-06-29T19:17:10.438726Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"256.203277ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-06-29T19:17:10.438763Z","caller":"traceutil/trace.go:171","msg":"trace[723333598] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12563; }","duration":"462.445328ms","start":"2025-06-29T19:17:09.976302Z","end":"2025-06-29T19:17:10.438749Z","steps":["trace[723333598] 'agreement among raft nodes before linearized reading'  (duration: 462.346013ms)"],"step_count":1}
{"level":"info","ts":"2025-06-29T19:17:10.438772Z","caller":"traceutil/trace.go:171","msg":"trace[368188130] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:12563; }","duration":"256.271022ms","start":"2025-06-29T19:17:10.182487Z","end":"2025-06-29T19:17:10.438758Z","steps":["trace[368188130] 'agreement among raft nodes before linearized reading'  (duration: 256.172685ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:10.438803Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:09.976288Z","time spent":"462.503018ms","remote":"127.0.0.1:55914","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-06-29T19:17:10.438717Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"313.187647ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-29T19:17:10.438853Z","caller":"traceutil/trace.go:171","msg":"trace[641289157] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:12563; }","duration":"313.349331ms","start":"2025-06-29T19:17:10.125490Z","end":"2025-06-29T19:17:10.438840Z","steps":["trace[641289157] 'agreement among raft nodes before linearized reading'  (duration: 313.178987ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:10.438902Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:10.125474Z","time spent":"313.412747ms","remote":"127.0.0.1:56016","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":0,"response size":29,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true "}
{"level":"warn","ts":"2025-06-29T19:17:11.587155Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.056151ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-29T19:17:11.587217Z","caller":"traceutil/trace.go:171","msg":"trace[209277634] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12565; }","duration":"191.139752ms","start":"2025-06-29T19:17:11.396063Z","end":"2025-06-29T19:17:11.587202Z","steps":["trace[209277634] 'range keys from in-memory index tree'  (duration: 190.908716ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:11.587297Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"327.97098ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-29T19:17:11.587343Z","caller":"traceutil/trace.go:171","msg":"trace[435954509] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12565; }","duration":"328.054441ms","start":"2025-06-29T19:17:11.259278Z","end":"2025-06-29T19:17:11.587333Z","steps":["trace[435954509] 'range keys from in-memory index tree'  (duration: 327.852598ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:11.587375Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:11.259247Z","time spent":"328.117159ms","remote":"127.0.0.1:55914","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-06-29T19:17:13.001863Z","caller":"traceutil/trace.go:171","msg":"trace[1798650427] transaction","detail":"{read_only:false; response_revision:12566; number_of_response:1; }","duration":"494.92628ms","start":"2025-06-29T19:17:12.506917Z","end":"2025-06-29T19:17:13.001845Z","steps":["trace[1798650427] 'process raft request'  (duration: 494.724927ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-29T19:17:13.001983Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-29T19:17:12.506898Z","time spent":"495.016027ms","remote":"127.0.0.1:56102","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12564 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-06-29T19:18:03.416238Z","caller":"etcdserver/server.go:1200","msg":"failed to revoke lease","lease-id":"70cc97bcabff00fe","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:18:03.416217Z","caller":"etcdserver/server.go:1200","msg":"failed to revoke lease","lease-id":"70cc97bcabff00fe","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:18:03.393634Z","caller":"etcdserver/server.go:1200","msg":"failed to revoke lease","lease-id":"70cc97bcabff012f","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:18:03.293716Z","caller":"etcdserver/v3_server.go:961","msg":"failed to get read index from Raft","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:18:03.343763Z","caller":"etcdserver/server.go:1239","msg":"Failed to check current member's leadership","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:18:22.011363Z","caller":"etcdserver/server.go:1200","msg":"failed to revoke lease","lease-id":"70cc97bcabff012f","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:18:21.872828Z","caller":"etcdserver/server.go:1179","msg":"Ignore the lease revoking request because current member isn't a leader","local-member-id":12593026477526642892}
{"level":"warn","ts":"2025-06-29T19:18:22.011363Z","caller":"etcdserver/server.go:1200","msg":"failed to revoke lease","lease-id":"70cc97bcabff00fe","error":"context deadline exceeded"}
{"level":"warn","ts":"2025-06-29T19:19:09.996543Z","caller":"etcdserver/v3_server.go:932","msg":"timed out waiting for read index response (local node might have slow network)","timeout":"7s"}


==> etcd [ebe86543c439] <==
{"level":"warn","ts":"2025-06-29T21:15:01.783288Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-06-29T21:15:01.783632Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-06-29T21:15:01.783648Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-06-29T21:15:01.783723Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-06-29T21:15:01.783743Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-29T21:15:01.783752Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-29T21:15:01.783788Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-29T21:15:01.787799Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-29T21:15:01.789233Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-06-29T21:15:01.820481Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"29.90979ms"}
{"level":"info","ts":"2025-06-29T21:15:01.903698Z","caller":"etcdserver/server.go:513","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.9 kB"}
{"level":"info","ts":"2025-06-29T21:15:01.903736Z","caller":"etcdserver/server.go:526","msg":"recovered v3 backend from snapshot","backend-size-bytes":2568192,"backend-size":"2.6 MB","backend-size-in-use-bytes":1323008,"backend-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-06-29T21:15:01.932403Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":15649}
{"level":"info","ts":"2025-06-29T21:15:01.932612Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-06-29T21:15:01.932634Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2025-06-29T21:15:01.932643Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 4, commit: 15649, applied: 10001, lastindex: 15649, lastterm: 4]"}
{"level":"info","ts":"2025-06-29T21:15:01.933099Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-29T21:15:01.933130Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-29T21:15:01.933141Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-06-29T21:15:01.937894Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-06-29T21:15:01.943517Z","caller":"mvcc/kvstore.go:348","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":12130}
{"level":"info","ts":"2025-06-29T21:15:01.944289Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":12577}
{"level":"info","ts":"2025-06-29T21:15:01.944307Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":15648}
{"level":"info","ts":"2025-06-29T21:15:01.948779Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-29T21:15:01.953757Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-06-29T21:15:01.954001Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-06-29T21:15:01.954038Z","caller":"etcdserver/server.go:866","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-29T21:15:01.954110Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-06-29T21:15:01.954183Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-29T21:15:01.954235Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-29T21:15:01.954271Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-29T21:15:01.955904Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-29T21:15:01.958323Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-29T21:15:01.958617Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-29T21:15:01.958647Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-29T21:15:01.958657Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-29T21:15:01.958679Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-29T21:15:02.834135Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2025-06-29T21:15:02.834202Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2025-06-29T21:15:02.834246Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-06-29T21:15:02.834269Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2025-06-29T21:15:02.834315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-06-29T21:15:02.834336Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2025-06-29T21:15:02.834353Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-06-29T21:15:02.840606Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-29T21:15:02.840632Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-29T21:15:02.840664Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-29T21:15:02.841315Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-29T21:15:02.841334Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-29T21:15:02.842943Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-29T21:15:02.842943Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-29T21:15:02.843598Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-29T21:15:02.843596Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 21:24:56 up 15 min,  0 users,  load average: 1.27, 1.75, 1.25
Linux minikube 6.12.10-76061203-generic #202412060638~1748542656~22.04~663e4dc SMP PREEMPT_DYNAMIC Thu M x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [ba10003638a4] <==
I0629 17:11:20.122371       1 local_available_controller.go:156] Starting LocalAvailability controller
I0629 17:11:20.122383       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0629 17:11:20.122493       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0629 17:11:20.122525       1 aggregator.go:169] waiting for initial CRD sync...
I0629 17:11:20.122964       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0629 17:11:20.122988       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0629 17:11:20.123079       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0629 17:11:20.123291       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0629 17:11:20.123363       1 controller.go:142] Starting OpenAPI controller
I0629 17:11:20.123433       1 controller.go:90] Starting OpenAPI V3 controller
I0629 17:11:20.123495       1 naming_controller.go:299] Starting NamingConditionController
I0629 17:11:20.123537       1 establishing_controller.go:81] Starting EstablishingController
I0629 17:11:20.123588       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0629 17:11:20.123628       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0629 17:11:20.123681       1 crd_finalizer.go:269] Starting CRDFinalizer
I0629 17:11:20.125052       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0629 17:11:20.128726       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0629 17:11:20.129421       1 repairip.go:200] Starting ipallocator-repair-controller
I0629 17:11:20.129449       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0629 17:11:20.173225       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0629 17:11:20.188703       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0629 17:11:20.188767       1 policy_source.go:240] refreshing policies
I0629 17:11:20.217443       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0629 17:11:20.217488       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0629 17:11:20.217884       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0629 17:11:20.217918       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0629 17:11:20.218076       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0629 17:11:20.222297       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0629 17:11:20.222420       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0629 17:11:20.224798       1 cache.go:39] Caches are synced for LocalAvailability controller
I0629 17:11:20.226047       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0629 17:11:20.228893       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0629 17:11:20.229399       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0629 17:11:20.229484       1 aggregator.go:171] initial CRD sync complete...
I0629 17:11:20.229499       1 autoregister_controller.go:144] Starting autoregister controller
I0629 17:11:20.229510       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0629 17:11:20.229518       1 cache.go:39] Caches are synced for autoregister controller
I0629 17:11:20.229518       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0629 17:11:20.250943       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 17:11:20.269994       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0629 17:11:20.271391       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0629 17:11:21.120960       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0629 17:11:21.437106       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0629 17:11:21.438467       1 controller.go:667] quota admission added evaluator for: endpoints
I0629 17:11:21.445796       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0629 17:11:23.461374       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0629 17:11:23.660845       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0629 17:11:23.820459       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 17:21:20.145285       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 17:31:20.146104       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 17:41:20.157237       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 17:51:20.148932       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 18:01:20.165363       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 18:33:02.201146       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 18:43:02.199255       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 18:53:02.202025       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0629 19:00:36.776438       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0629 19:00:40.051308       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0629 19:03:02.230704       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 19:13:02.201672       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-apiserver [df58b0bc3bb3] <==
I0629 21:15:03.363719       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0629 21:15:03.363730       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0629 21:15:03.363737       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0629 21:15:03.363686       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0629 21:15:03.363792       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0629 21:15:03.363808       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0629 21:15:03.363813       1 establishing_controller.go:81] Starting EstablishingController
I0629 21:15:03.364462       1 naming_controller.go:299] Starting NamingConditionController
I0629 21:15:03.364466       1 controller.go:142] Starting OpenAPI controller
I0629 21:15:03.364493       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0629 21:15:03.364522       1 crd_finalizer.go:269] Starting CRDFinalizer
I0629 21:15:03.364466       1 controller.go:90] Starting OpenAPI V3 controller
I0629 21:15:03.364746       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0629 21:15:03.364755       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0629 21:15:03.364756       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0629 21:15:03.364821       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0629 21:15:03.365074       1 controller.go:119] Starting legacy_token_tracking_controller
I0629 21:15:03.365086       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0629 21:15:03.365428       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0629 21:15:03.365445       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0629 21:15:03.365490       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0629 21:15:03.365558       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0629 21:15:03.365689       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0629 21:15:03.365706       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0629 21:15:03.367265       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0629 21:15:03.372538       1 repairip.go:200] Starting ipallocator-repair-controller
I0629 21:15:03.372565       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0629 21:15:03.387673       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0629 21:15:03.387814       1 policy_source.go:240] refreshing policies
I0629 21:15:03.464465       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0629 21:15:03.464502       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0629 21:15:03.464567       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0629 21:15:03.464591       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0629 21:15:03.464612       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0629 21:15:03.464630       1 aggregator.go:171] initial CRD sync complete...
I0629 21:15:03.464656       1 autoregister_controller.go:144] Starting autoregister controller
I0629 21:15:03.464664       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0629 21:15:03.464672       1 cache.go:39] Caches are synced for autoregister controller
I0629 21:15:03.464703       1 cache.go:39] Caches are synced for LocalAvailability controller
I0629 21:15:03.465236       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0629 21:15:03.465489       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0629 21:15:03.465895       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0629 21:15:03.465928       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0629 21:15:03.467635       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0629 21:15:03.472877       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
E0629 21:15:03.472879       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0629 21:15:03.477300       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 21:15:03.479367       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0629 21:15:03.481588       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
E0629 21:15:03.485082       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: c3e18f56-33f0-4a54-a7dd-3aa3b668e0b7, UID in object meta: "
I0629 21:15:04.366914       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0629 21:15:04.586383       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0629 21:15:06.736148       1 controller.go:667] quota admission added evaluator for: endpoints
I0629 21:15:06.835605       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0629 21:15:06.987403       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0629 21:15:07.090168       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 21:15:07.096127       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0629 21:15:07.187677       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0629 21:21:08.639835       1 alloc.go:328] "allocated clusterIPs" service="default/inference-service" clusterIPs={"IPv4":"10.105.85.227"}
I0629 21:21:08.688358       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [c036a04edc60] <==
I0629 17:11:23.355398       1 controllermanager.go:778] "Started controller" controller="node-lifecycle-controller"
I0629 17:11:23.355414       1 controllermanager.go:756] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0629 17:11:23.355534       1 node_lifecycle_controller.go:453] "Sending events to api server" logger="node-lifecycle-controller"
I0629 17:11:23.355595       1 node_lifecycle_controller.go:464] "Starting node controller" logger="node-lifecycle-controller"
I0629 17:11:23.355604       1 shared_informer.go:350] "Waiting for caches to sync" controller="taint"
I0629 17:11:23.359393       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0629 17:11:23.367642       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0629 17:11:23.369957       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0629 17:11:23.372364       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0629 17:11:23.374765       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0629 17:11:23.376379       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0629 17:11:23.377154       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0629 17:11:23.379231       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0629 17:11:23.388765       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0629 17:11:23.390924       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0629 17:11:23.396485       1 shared_informer.go:357] "Caches are synced" controller="node"
I0629 17:11:23.396686       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0629 17:11:23.396751       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0629 17:11:23.396767       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0629 17:11:23.396777       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0629 17:11:23.399768       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0629 17:11:23.402934       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0629 17:11:23.405213       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0629 17:11:23.405247       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0629 17:11:23.405496       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0629 17:11:23.405689       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0629 17:11:23.406782       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0629 17:11:23.410142       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0629 17:11:23.436142       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0629 17:11:23.455696       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0629 17:11:23.455764       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0629 17:11:23.455978       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0629 17:11:23.456090       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0629 17:11:23.456155       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0629 17:11:23.456563       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0629 17:11:23.457563       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0629 17:11:23.459057       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0629 17:11:23.461449       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0629 17:11:23.464232       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0629 17:11:23.466475       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0629 17:11:23.466605       1 shared_informer.go:357] "Caches are synced" controller="job"
I0629 17:11:23.474322       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0629 17:11:23.505917       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0629 17:11:23.606793       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0629 17:11:23.680287       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0629 17:11:23.705082       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0629 17:11:23.755985       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0629 17:11:23.755982       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0629 17:11:23.757161       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0629 17:11:23.757170       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0629 17:11:23.759406       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0629 17:11:23.760291       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0629 17:11:23.768455       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0629 17:11:23.794420       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0629 17:11:24.179802       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0629 17:11:24.256594       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0629 17:11:24.256614       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0629 17:11:24.256626       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
E0629 19:00:36.795311       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0629 19:00:40.053174       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-controller-manager [dba765f3181e] <==
I0629 21:15:06.432320       1 shared_informer.go:350] "Waiting for caches to sync" controller="expand"
I0629 21:15:06.482277       1 controllermanager.go:778] "Started controller" controller="persistentvolume-protection-controller"
I0629 21:15:06.482291       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="kube-apiserver-serving-clustertrustbundle-publisher-controller" requiredFeatureGates=["ClusterTrustBundle"]
I0629 21:15:06.482371       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0629 21:15:06.482434       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0629 21:15:06.582387       1 controllermanager.go:778] "Started controller" controller="validatingadmissionpolicy-status-controller"
I0629 21:15:06.582424       1 shared_informer.go:350] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
I0629 21:15:06.586343       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0629 21:15:06.590306       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0629 21:15:06.590621       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0629 21:15:06.591066       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0629 21:15:06.591097       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0629 21:15:06.592969       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0629 21:15:06.594306       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0629 21:15:06.597957       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0629 21:15:06.603746       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0629 21:15:06.606003       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0629 21:15:06.607097       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0629 21:15:06.608383       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0629 21:15:06.613084       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0629 21:15:06.614574       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0629 21:15:06.614728       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0629 21:15:06.627475       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0629 21:15:06.631848       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0629 21:15:06.633005       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0629 21:15:06.633017       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0629 21:15:06.637408       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0629 21:15:06.639377       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0629 21:15:06.646952       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0629 21:15:06.653896       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0629 21:15:06.655073       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0629 21:15:06.679258       1 shared_informer.go:357] "Caches are synced" controller="node"
I0629 21:15:06.679322       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0629 21:15:06.679359       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0629 21:15:06.679365       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0629 21:15:06.679372       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0629 21:15:06.683344       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0629 21:15:06.683369       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0629 21:15:06.683390       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0629 21:15:06.683394       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0629 21:15:06.683485       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0629 21:15:06.686596       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0629 21:15:06.687319       1 shared_informer.go:357] "Caches are synced" controller="job"
I0629 21:15:06.688494       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0629 21:15:06.690794       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0629 21:15:06.697153       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0629 21:15:06.718586       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0629 21:15:06.782714       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0629 21:15:06.782838       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0629 21:15:06.782909       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0629 21:15:06.782978       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0629 21:15:06.826422       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0629 21:15:06.832146       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0629 21:15:06.944055       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0629 21:15:06.986623       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0629 21:15:06.989931       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0629 21:15:07.399134       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0629 21:15:07.423795       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0629 21:15:07.423812       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0629 21:15:07.423824       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [8be198d31281] <==
I0629 17:11:21.805948       1 server_linux.go:63] "Using iptables proxy"
I0629 17:11:22.080952       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0629 17:11:22.081068       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0629 17:11:22.116492       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0629 17:11:22.116541       1 server_linux.go:145] "Using iptables Proxier"
I0629 17:11:22.123251       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0629 17:11:22.129524       1 server.go:516] "Version info" version="v1.33.1"
I0629 17:11:22.129554       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0629 17:11:22.135416       1 config.go:199] "Starting service config controller"
I0629 17:11:22.135494       1 config.go:105] "Starting endpoint slice config controller"
I0629 17:11:22.135932       1 config.go:440] "Starting serviceCIDR config controller"
I0629 17:11:22.136350       1 config.go:329] "Starting node config controller"
I0629 17:11:22.136941       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0629 17:11:22.136953       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0629 17:11:22.136926       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0629 17:11:22.136973       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0629 17:11:22.238135       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0629 17:11:22.238175       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0629 17:11:22.238221       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0629 17:11:22.238142       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0629 19:18:28.964503       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:18:29.264322       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:18:28.964496       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"


==> kube-proxy [e534b63e6b37] <==
I0629 21:15:05.841666       1 server_linux.go:63] "Using iptables proxy"
I0629 21:15:05.977488       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0629 21:15:05.977636       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0629 21:15:06.012148       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0629 21:15:06.012195       1 server_linux.go:145] "Using iptables Proxier"
I0629 21:15:06.016667       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0629 21:15:06.021604       1 server.go:516] "Version info" version="v1.33.1"
I0629 21:15:06.021627       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0629 21:15:06.025193       1 config.go:199] "Starting service config controller"
I0629 21:15:06.025273       1 config.go:105] "Starting endpoint slice config controller"
I0629 21:15:06.025610       1 config.go:440] "Starting serviceCIDR config controller"
I0629 21:15:06.025695       1 config.go:329] "Starting node config controller"
I0629 21:15:06.026124       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0629 21:15:06.026131       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0629 21:15:06.026343       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0629 21:15:06.026132       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0629 21:15:06.127013       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0629 21:15:06.127040       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0629 21:15:06.127062       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0629 21:15:06.127048       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [97f5e6d9e8ac] <==
I0629 17:11:18.357593       1 serving.go:386] Generated self-signed cert in-memory
W0629 17:11:20.143962       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0629 17:11:20.144023       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0629 17:11:20.144039       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0629 17:11:20.144051       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0629 17:11:20.198457       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0629 17:11:20.198486       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0629 17:11:20.204302       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0629 17:11:20.204652       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0629 17:11:20.211282       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0629 17:11:20.210189       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0629 17:11:20.312867       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0629 19:19:04.997793       1 reflector.go:556] "Warning: watch ended with error" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:02.969957       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:03.216360       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:02.964662       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:03.219221       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:03.295402       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.082485       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:02.964929       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.800633       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.685270       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.745544       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.685270       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.685275       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.800642       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0629 19:19:05.800651       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"


==> kube-scheduler [d4d21952546b] <==
I0629 21:15:02.209165       1 serving.go:386] Generated self-signed cert in-memory
W0629 21:15:03.373989       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0629 21:15:03.374025       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0629 21:15:03.374039       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0629 21:15:03.374051       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0629 21:15:03.396589       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0629 21:15:03.396607       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0629 21:15:03.400262       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0629 21:15:03.400495       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0629 21:15:03.400547       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0629 21:15:03.401362       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0629 21:15:03.502105       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.708414    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.708452    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.708550    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.721121    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.721198    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.721209    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.740640    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.748307    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.748332    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.759241    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.759266    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.766174    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: I0629 21:15:03.766196    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 29 21:15:03 minikube kubelet[1675]: E0629 21:15:03.772911    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.103627    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jun 29 21:15:04 minikube kubelet[1675]: E0629 21:15:04.112323    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.529427    1675 apiserver.go:52] "Watching apiserver"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.541556    1675 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.584842    1675 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/f91ba068-640a-451b-9592-a8095f4f1536-tmp\") pod \"storage-provisioner\" (UID: \"f91ba068-640a-451b-9592-a8095f4f1536\") " pod="kube-system/storage-provisioner"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.584876    1675 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/3397e75e-93c1-45b4-8e8d-7c52ce1b00cf-xtables-lock\") pod \"kube-proxy-sfj65\" (UID: \"3397e75e-93c1-45b4-8e8d-7c52ce1b00cf\") " pod="kube-system/kube-proxy-sfj65"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.584896    1675 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/3397e75e-93c1-45b4-8e8d-7c52ce1b00cf-lib-modules\") pod \"kube-proxy-sfj65\" (UID: \"3397e75e-93c1-45b4-8e8d-7c52ce1b00cf\") " pod="kube-system/kube-proxy-sfj65"
Jun 29 21:15:04 minikube kubelet[1675]: I0629 21:15:04.713781    1675 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 29 21:15:04 minikube kubelet[1675]: E0629 21:15:04.721542    1675 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 29 21:15:06 minikube kubelet[1675]: I0629 21:15:06.770890    1675 scope.go:117] "RemoveContainer" containerID="b4cd3200457a6a70b7401512aaa2437badf1c81981d1a36b3d38f9d5bfcbf5d7"
Jun 29 21:15:06 minikube kubelet[1675]: I0629 21:15:06.771217    1675 scope.go:117] "RemoveContainer" containerID="39b6f7099fd61d77230a565042d9046cf79c70d00d793c453811210c56362102"
Jun 29 21:15:06 minikube kubelet[1675]: E0629 21:15:06.771378    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f91ba068-640a-451b-9592-a8095f4f1536)\"" pod="kube-system/storage-provisioner" podUID="f91ba068-640a-451b-9592-a8095f4f1536"
Jun 29 21:15:21 minikube kubelet[1675]: I0629 21:15:21.562143    1675 scope.go:117] "RemoveContainer" containerID="39b6f7099fd61d77230a565042d9046cf79c70d00d793c453811210c56362102"
Jun 29 21:21:08 minikube kubelet[1675]: I0629 21:21:08.916262    1675 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v27m4\" (UniqueName: \"kubernetes.io/projected/e66e411e-61b5-4006-8fef-700e6450351e-kube-api-access-v27m4\") pod \"inference-service-849f8d958f-flt55\" (UID: \"e66e411e-61b5-4006-8fef-700e6450351e\") " pod="default/inference-service-849f8d958f-flt55"
Jun 29 21:21:11 minikube kubelet[1675]: E0629 21:21:11.982945    1675 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:21:11 minikube kubelet[1675]: E0629 21:21:11.983742    1675 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:21:11 minikube kubelet[1675]: E0629 21:21:11.988469    1675 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:resnet,Image:resnet-infer,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:6001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v27m4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod inference-service-849f8d958f-flt55_default(e66e411e-61b5-4006-8fef-700e6450351e): ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 29 21:21:11 minikube kubelet[1675]: E0629 21:21:11.990288    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ErrImagePull: \"Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:21:12 minikube kubelet[1675]: E0629 21:21:12.061369    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:21:29 minikube kubelet[1675]: E0629 21:21:29.790912    1675 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:21:29 minikube kubelet[1675]: E0629 21:21:29.790968    1675 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:21:29 minikube kubelet[1675]: E0629 21:21:29.791095    1675 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:resnet,Image:resnet-infer,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:6001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v27m4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod inference-service-849f8d958f-flt55_default(e66e411e-61b5-4006-8fef-700e6450351e): ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 29 21:21:29 minikube kubelet[1675]: E0629 21:21:29.792307    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ErrImagePull: \"Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:21:44 minikube kubelet[1675]: E0629 21:21:44.562623    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:22:01 minikube kubelet[1675]: E0629 21:22:01.843100    1675 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:22:01 minikube kubelet[1675]: E0629 21:22:01.843159    1675 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:22:01 minikube kubelet[1675]: E0629 21:22:01.843268    1675 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:resnet,Image:resnet-infer,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:6001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v27m4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod inference-service-849f8d958f-flt55_default(e66e411e-61b5-4006-8fef-700e6450351e): ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 29 21:22:01 minikube kubelet[1675]: E0629 21:22:01.844413    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ErrImagePull: \"Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:22:16 minikube kubelet[1675]: E0629 21:22:16.562679    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:22:29 minikube kubelet[1675]: E0629 21:22:29.562362    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:22:44 minikube kubelet[1675]: E0629 21:22:44.688117    1675 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:22:44 minikube kubelet[1675]: E0629 21:22:44.688170    1675 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:22:44 minikube kubelet[1675]: E0629 21:22:44.688302    1675 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:resnet,Image:resnet-infer,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:6001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v27m4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod inference-service-849f8d958f-flt55_default(e66e411e-61b5-4006-8fef-700e6450351e): ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 29 21:22:44 minikube kubelet[1675]: E0629 21:22:44.690295    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ErrImagePull: \"Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:22:59 minikube kubelet[1675]: E0629 21:22:59.562488    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:23:14 minikube kubelet[1675]: E0629 21:23:14.562087    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:23:27 minikube kubelet[1675]: E0629 21:23:27.562553    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:23:40 minikube kubelet[1675]: E0629 21:23:40.562428    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:23:55 minikube kubelet[1675]: E0629 21:23:55.562023    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:24:17 minikube kubelet[1675]: E0629 21:24:17.933911    1675 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:24:17 minikube kubelet[1675]: E0629 21:24:17.933963    1675 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="resnet-infer:latest"
Jun 29 21:24:17 minikube kubelet[1675]: E0629 21:24:17.934076    1675 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:resnet,Image:resnet-infer,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:6001,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v27m4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod inference-service-849f8d958f-flt55_default(e66e411e-61b5-4006-8fef-700e6450351e): ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 29 21:24:17 minikube kubelet[1675]: E0629 21:24:17.935252    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ErrImagePull: \"Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:24:29 minikube kubelet[1675]: E0629 21:24:29.562276    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:24:43 minikube kubelet[1675]: E0629 21:24:43.562045    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"
Jun 29 21:24:54 minikube kubelet[1675]: E0629 21:24:54.562111    1675 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"resnet\" with ImagePullBackOff: \"Back-off pulling image \\\"resnet-infer\\\": ErrImagePull: Error response from daemon: pull access denied for resnet-infer, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/inference-service-849f8d958f-flt55" podUID="e66e411e-61b5-4006-8fef-700e6450351e"


==> storage-provisioner [39b6f7099fd6] <==
I0629 21:15:05.616240       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0629 21:15:06.659607       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: no route to host


==> storage-provisioner [4e2a4065be90] <==
W0629 21:23:58.117964       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:23:58.138921       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:00.142435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:00.166503       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:02.169760       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:02.175534       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:04.179848       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:04.185981       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:06.190061       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:06.196114       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:08.200779       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:08.206420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:10.210871       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:10.215693       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:12.219307       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:12.224667       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:14.227712       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:14.233963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:16.237854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:16.244186       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:18.249814       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:18.255894       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:20.258912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:20.264610       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:22.267889       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:22.273935       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:24.276543       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:24.283429       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:26.286775       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:26.292280       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:28.295665       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:28.303269       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:30.306624       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:30.312004       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:32.315626       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:32.324053       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:34.326708       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:34.332821       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:36.340765       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:36.345759       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:38.347977       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:38.354253       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:40.357288       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:40.364435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:42.367199       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:42.372513       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:44.375473       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:44.382377       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:46.384435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:46.388922       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:48.391356       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:48.398838       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:50.401476       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:50.407104       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:52.410066       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:52.416586       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:54.419837       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:54.428005       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:56.430516       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0629 21:24:56.435524       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

